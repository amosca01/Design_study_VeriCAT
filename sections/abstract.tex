\section{Abstract}

%Explainable AI (XAI) tools and research efforts seek to provide insight about “black box” AI/ML systems. Many of these efforts, however, overlook the UI/UX (user interface / user experience) aspects of this challenge. Extracting the right information from and about models is crucial, but it is only part of the challenge of explainability; this information must also be presented to someone through an effective interface.

In this paper, we describe the design, development, and evaluation of VeriCAT, a system that visualizes output of a machine translation (MT) quality estimation (QE) model. In 2017, an incorrect machine translation of a Facebook post led to the erroneous arrest of the author. We design VeriCAT with the intent of preventing situations such as this. VeriCAT, short for verification of computer-assisted translation, displays the likely inaccuracy of a machine translation. This enables users to determine whether to trust a specific machine-translated sentence. VeriCAT predicts the sentence-level quality of translations from Russian into English by the FairSeq MT model. Its user interface shows this predicted quality as a quality score for each translated sentence. We evaluate VeriCAT with a quantitative user study to measure how the tool impacts participants’ ability to identify poor quality machine translations. Our evaluation shows the tool significantly increases participants' accuracy in identifying poor quality machine translations. Moreover, we find participants perform their task as accurately with VeriCAT QE scores as with ground truth quality scores. Finally, we provide lessons learned from this study to inform future work.  

%suggest ways in which future work can leverage our UI evaluation to iterate on VeriCAT as an XAI system system. 
