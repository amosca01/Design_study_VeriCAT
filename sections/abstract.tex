\section{Abstract}

%Explainable AI (XAI) tools and research efforts seek to provide insight about “black box” AI/ML systems. Many of these efforts, however, overlook the UI/UX (user interface / user experience) aspects of this challenge. Extracting the right information from and about models is crucial, but it is only part of the challenge of explainability; this information must also be presented to someone through an effective interface.

In this paper, we describe the design, development and evaluation of VeriCAT, a tool that combines a Machine Translation (MT) Quality Estimation (QE) model with a simple-to-use interface. The purpose of VeriCAT is to help users understand the limitations of machine translation and determine whether or not to trust a specific machine translated sentence. VeriCAT predicts the translation quality of sentences translated from Russian into English with the FairSeq MT model, and shows users sentence-level quality scores for translated passages of text. We evaluate VeriCAT with a quantitative user study that measures how the tool impacts users’ ability to perform a human-AI collaborative task. Additionally, we evaluate VeriCAT qualitatively with expert feedback. Both evaluations show VeriCAT is useful for its intended purpose. In the discussion section we suggest ways in which future work can iterate on VeriCAT with a user-centered design approach. 
