\section{Introduction}

In 2017 Facebook's machine translation (MT) algorithm incorrectly translated a construction worker's Arabic-language post. The original post said ``good morning" in Arabic, but was erroneously translated into Hebrew as ``attack them", leading to the worker's arrest and several hours of questioning. Notably, no Arabic-speakers were asked to verify the machine translation of the post leading up to the arrest~\cite{hernFacebook2017}. For many users of machine translation, it is easy to forget that translated output is susceptible to error and, as illustrated by this situation, some translation errors can lead to severe consequences. 
%Analysts might use machine translation in situations where human translators are in short supply. To avoid potentially negative consequences of erroneous translations, analysts would benefit from tools that help them  
%asses the quality of specific passages of translated text.  

Our goal in this work is to develop and evaluate a tool to meet this need--helping users determine when, and whether, to trust machine translation. 
Initially, we looked to Explainable AI (XAI) tools and approaches for guidance.
%However, we find that most XAI initiatives are geared towards helping AI experts debug their models, as opposed to helping subject matter experts effectively and efficiently use model output~\cite{brennen2020What}. As such, most state of the art XAI solutions are not particularly helpful for analysts without substantial expertise in AI \remco{the last two sentences are difficult to defend -- a reviewer just needs one counter example, however shitty, to discount the motivation}.  
Ribero, Selbst \& Barocas, and Mittelstadt et. al. discuss multiple approaches to XAI: 
%\remco{replace with ``XXX et al. have suggested that'' -- more defensible} 
(1) using approximate models, (2) allowing users to explore how various input correlates with different outputs, and (3) replacing an opaque "black-box" model with a simpler “white-box" model ~\cite{RiberoLIME2016, SelbstBarocasIntuitive2018, MittelstadtRussellExplain2019}. However, none of these approaches is sufficient for our use case -- helping analysts determine when, and whether, to trust machine translation.
%\remco{this is a good sentence. It's the first time that the goal of the project is clearly stated. This sentence, more formalized (in terms of the actions that an analyst would need to do, should be put into the paragraph above}. 

Using an approximate MT model would significantly compromise translation accuracy%(if that were not the case we would use the simpler model for translation in general)
. Seeing pairs of foreign-language and translated text would be meaningless to non-speakers of that foreign language. And we know of no ``white-box" method for machine translation. \remco{this paragraph is great! Defend the three categories of XAI and this is a powerful motivation for why your work is needed.}

%\remco{Move the ``in this paper...'' paragraph up to here. Flow-wise, the motivation has been established. Now give the reader the solution. The next paragraph on QE is part of the solution of ``in this paper.'' Defending it prematurely feels apologetic.}

In this paper, we present and evaluate VeriCAT, which stands for verification of computer-assisted translation. VeriCAT is designed to help those using machine translation assess the quality of translated text. In particular, we focus on text snippets that have been translated from Russian into English by the FairSeq model ~\cite{ott-etal-2019-fairseq}. %In particular, VeriCAT aims to help users determine whether a particular MT sentence is trustworthy. For our particular use case, we build VeriCAT such that it provides ``explanations" for text translated from Russian $\rightarrow$ English by the FairSeq model~\cite{ott-etal-2019-fairseq}. 
VeriCAT consists of a Quality Estimation (QE) model combined with an easy-to-use interface. The objective of QE is to train a machine learning model to predict a quality score for translated text that is similar to what a human would assign to that translation~\cite{mauvcec2019machine}. VeriCAT's QE model is a trained version of OpenKiwi’s predictor-estimator QE~\cite{Kim2017PredictorEstimatorUM}. VeriCAT's interface shows users a predicted quality score (some valus out of a possible 100) for each individual sentence of machine-translated text (Figure \ref{fig:p3_predicted_quality}). %The main objective of the system is to provide users with additional information that they can use to determine whether a particular MT sentence is trustworthy.  %Furthermore, we run an empirical user study to see how users respond to VeriCAT, and if their individual differences affect how they use the tool. 

VeriCAT is novel in that it uses the output of a Quality Estimation model to provide context to analysts. Typically, QE is used by developers of MT models for validation and model improvement. However, we believe predicted quality scores can benefit analysts as well, by helping them determine when and whether to trust a particular MT sentence. While many MT accuracy metrics (such as BLEU score \cite{papineni-etal-2002-bleu}) provide information about the accuracy of a MT model in general, QE serves as a metric for \textit{individual sentences}. %Our approach differs from typical approaches to XAI in that it uses the output of one opaque model (the QE model) to provide context for the output of another opaque model (the MT model).


%In addition to turning to XAI, we spoke with analysts to better understand their needs around MT text. VeriCAT is novel in that it uses QE scores as an ``explanation" for MT text. While many common MT accuracy metrics (such as BLEU score \cite{papineni-etal-2002-bleu}) provide information about the accuracy of a MT model in general, QE serves as a metric for \textit{individual sentences}.The objective of QE is to train a machine learning model to predict a quality score for translated text that is similar to what a human would assign to the translation~\cite{mauvcec2019machine}. QE is typically used by developers of MT models for validation and model improvement. However, we believe QE can benefit analysts as well, by providing important contextual information about when and whether to trust a particular MT sentence. Our approach differs from typical approaches to XAI in that it uses the output of one opaque model (the QE model) to provide context for the output of another opaque model (the MT model). %However, we believe a quality score is a useful and intuitive way to help analysts determine whether MT text should be trusted or not.  

%Because analysts are in high demand, 
We evaluate VeriCAT with a quantitative %crowdsourced 
user study, where users are asked to perform an analogous task to that which motivated the development of VeriCAT. Participants are shown a passage of text translated from Russian $\rightarrow$ to English via the FairSeq model~\cite{ott-etal-2019-fairseq}. Participants are informed that they will be asked to answer two comprehension questions based on the text and are given the opportunity to request a human translation of any (or none) of the sentences in the passage before seeing the comprehension questions. We advise participants to select the sentence with the poorest quality translation for re-translation by a human, and we score participants based on whether they actually choose the lowest quality translation, as measured by a Direct Assessment score. \andrea{add sentence explaining DA score}

Our study shows that participants who have access to VeriCAT's predicted quality scores more frequently select the lowest quality translation (i.e. they perform better on the user study task), compared to participants who do not have VeriCAT's estimated quality scores. Moreover, we find correlations between participants' familiarity with MT tools and their self-rated expertise in AI and MT and their performance on the task. 

%Using QE as an explanation for MT text would be particularly useful in situations such as the Facebook scenario described above, where fluency of a MT is not an appropriate proxy measure for quality. However, there are drawbacks to using quality estimation as an explanation for MT. Namely, that QE models are also imperfect and there is no research showing whether users will heed quality scores over their own intuition. 

%Andrea stopped editing here ... on second read through as well

%\remco{need another paragraph or two to get into the meat of what VeriCAT is and/or what the user study is about. As a reader, at this point I'm still not 100\% sure what the contribution of the paper is... (1) is it VeriCat as a system? If so, it feels a little weird in that there's no declaration that VeriCat solves the problem posed in the motivation paragraph. (2) Related, if the contribution is VeriCat, then it's hard to believe that there are no other QE algorithms / software for MT. I feel like I have seen confidence bars before with a translation? Regardless, a quick google scholar search finds a patent on ``Method and apparatus for automated measurement of quality for machine translation''. So I imagine that others or out there? If so, why VeriCat and not other existing algorithms/software? (if other algorithms exist, then the motivation of comparing VeriCat to LIME isn't quite right) (3) is it the idea of using QE? If so, VeriCat is a means to an end. Then you need to say more about how you test the value of QE. (4) Personally, I feel like the contribution ought to be an integrated system of VeriCat + visualization interface?}

In the following sections we describe the design of VeriCAT. We briefly explain the QE model behind the system, and evaluate the system through a quantitative user study. Finally, we provide lessons learned from the design and evaluation of VeriCAT. % as well as a proposal for future work that incorporates users study results to iterate on the VeriCAT system.
In summary, we contribute the following: 

\begin{enumerate}
    \item The VeriCAT system which uses QE to help users decide if and when to trust sentences of machine translated text.   
   % \item \remco{possibly claim an ``iterative deisgn'' process that result in the interface for VeriCat? This is a little more compelling than the ``lessons learned'' at the end as a contribution but serves a similar goal? (Also, if this is established as a contribution, then there is more ground to claim UXAI as a lessons learned?)}
    \item An evaluation of VeriCAT demonstrating that it significantly improves participants' performance in identifying poor quality machine translations, and that differences between participants may effect how much they benefit from VeriCAT.
    \item Design implications and lessons learned from the design and evaluation of VeriCAT. 
\end{enumerate}
