\section{Introduction}

In 2017 Facebook's machine translation (MT) algorithm incorrectly translated a construction worker's Arabic-language post. The original post said ``good morning" in Arabic, but was erroneously translated into Hebrew as ``attack them", leading to the worker's arrest and several hours of questioning. Notably, no Arabic-speakers were asked to verify the machine translation of the post leading up to the arrest~\cite{hernFacebook2017}. For many users of machine translation, it is easy to forget that translated output is susceptible to error and, as illustrated by this situation, some translation errors can lead to severe consequences. 

In national security contexts, analysts might use machine translation in situations where human translators are in short supply. To avoid potentially negative consequences of erroneous translations, analysts would benefit from tools that help them understand the limitations of machine translation models and asses the quality of specific passages of translated text.  

Our goal in this work is to develop and evaluate a tool to meet this need: to help analysts understand when MT text is of low quality and should be verified by a human before it is acted upon. Given that MT is an instance of AI, we look to Explainable AI (XAI) tools and approaches for guidance. However, we find that most XAI initiatives are geared towards helping AI experts debug their models, as opposed to helping subject matter experts effectively and efficiently use model output~\cite{brennen2020What}. As such, most state of the art XAI solutions are not particularly helpful for analysts without substantial expertise in AI.  

Typical approaches for XAI can be broken into three categories: (1) use an approximate model, (2) allow users to explore various input-output pairs for the model, and (3) build a simpler ``white-box" version of a complicated model \cite{RiberoLIME2016, SelbstBarocasIntuitive2018, MittelstadtRussellExplain2019}. None of these general approaches is sufficient for our use case -- helping analysts determine when, and whether, to trust machine translation. Using an approximate MT model would significantly compromise translation accuracy (if that were not the case we would use the simpler model for translation in general). Seeing pairs of foreign-language and translated text would be meaningless to non-speakers of that foreign language. And there is no ``white-box" method for machine translation.      

%In addition to turning to XAI, we spoke with analysts to better understand their needs around MT text. 
Based on our understanding of this problem, we believe the best option for providing important contextual information to help analysts effectively utilize MT output is via quality estimation (QE). QE is typically used by developers of MT models for validation and model improvement. The objective is to train a machine learning model to predict a quality score for translated text that is similar to what a human would assign to the translation\cite{mauvcec2019machine}.  

Quality Estimation differs from typical approaches to XAI in that it uses the output of one opaque model (the QE model) to provide context for the output of another opaque model (the MT model). However, we decided to leverage QE as a means of "explaining" the output of MT models to analysts, as we believe a Quality Score is a useful and intuitive way to help analysts determine whether MT text should be trusted or not.    

%Using QE as an explanation for MT text would be particularly useful in situations such as the Facebook scenario described above, where fluency of a MT is not an appropriate proxy measure for quality. However, there are drawbacks to using quality estimation as an explanation for MT. Namely, that QE models are also imperfect and there is no research showing whether users will heed quality scores over their own intuition. 

%Andrea stopped editing here

In this paper, we present and evaluate VeriCAT, an XAI system that uses QE as an ``explanation" for text translated from Russian -> English by the FairSeq model. For a given passage of MT text, VeriCAT shows the user sentence-level QEs for each translated sentence in the passage. We generate QEs via OpenKiwi’s predictor-estimator QE\ab{cite same as 5.1}, and display them to the user as scores out of 100 (Figure \ref{fig:p3_predicted_quality}). The main objective of the system is to provide users with additional information that they can use to determine if a particular MT sentence is trustworthy, or if it should be verified by a human before being acted up. Furthermore, we run an empirical user study to see how users respond to VeriCAT, and if their individual differences affect how they use the tool.  

In the following sections we describe the design requirements that guided building VeriCAT. We briefly explain the QE model behind the system, and evaluate the system through a quantitative user study. Our quantitative study shows VeriCAT significantly improves users ability to identify MT text that is of low quality and should be re-translated by a human. Furthermore, we find that differences in participants' familiarity with MT tools and their self-rated expertise in AI and MT may effect how well they are able to identify untrustworthy MT text given QEs. Finally, we provide lessons learned from the design and evaluation of VeriCAT. % as well as a proposal for future work that incorporates users study results to iterate on the VeriCAT system.

In summary, we contribute the following: 
\begin{enumerate}
    \item The VeriCAT system which leverages OpenKiwi’s predictor-estimator QE to provide an element of ``explanation" to machine translation output. 
    \item An evaluation of VeriCAT demonstrating that it significantly improves participants' performance in identifying poor quality machine translations, and that differences between participants may effect their ability to benefit from VeriCAT.
    \item Lessons learned from the design and evaluation of VeriCAT including a call for more user-centered iterative design of XAI systems. 
\end{enumerate}
