\section{Conclusion}

Though increasingly widespread, machine translation (MT) is not perfect. Acting on miss-translations can lead to adverse events, but there are limited tools designed to help users of MT assess the quality of translations. In this paper we present VeriCAT, a system %built to help users of MT text understand when and whether MT text can be trusted. The VeriCAT system 
that combines a Machine Translation (MT) Quality Estimation (QE) model with a simple-to-use interface. The QE model is a finetuned version of OpenKiwi's predictor-estimator QE for Russian $\rightarrow$ English translations via the FairSeq model. VeriCAT's interface displays predicted quality scores alongside each sentence of MT text. We evaluate VeriCAT with a quantitative user study that measures how VeriCAT's quality scores impact usersâ€™ ability to identify poor quality MT text. Our evaluation shows VeriCAT's predicted quality scores significantly improve participants' accuracy in identifying poor quality machine translated text compared to the baseline condition. We also find that participants who see machine-predicted quality scores perform as well as those who see human-generated quality scores. 

