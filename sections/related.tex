\section{Related Work}

Despite the fact that advanced MT models typically include extremely opaque RNNs and that their output is meant to be used in human-AI collaborative tasks\cite{mauvcec2019machine}, there has been very little research on the usability of MT output and how XAI may help. Our work centers on the meeting of these two fields; namely in identifying how XAI may be used to improve the utility of MT output. To set the stage for out work we provide high-level summaries of existing work in MT and XAI in the following sections.  

\subsection{Machine Translation} 

The study of MT began in the 1950's, but it was not until recently that significant advances were made which greatly improved accuracy and usability of MT. Despite these advances, human translators still outperform MT in terms of accuracy and preserving the original meaning of translated text\cite{mauvcec2019machine}. As MT becomes more and more mainstream problems can arise when those unfamiliar with MT are not given context to understand the quality of translations. One way to address this problem is with QE.  

There are many ways to perform QE for MT. Numerous automatic metrics aim to approximate human judgement. Some common examples include BLEU, NIST, METEOR, and TER. Additionally, there are some human in the loop automatic judgements. For instance HTER (human-mediated translation error rate), which attempts to capture the number of post edits made to a MT by a human translator\cite{mauvcec2019machine}. On the other hand there are human judgements which can come from direct assessment of aspects of the translation such as fluency and adequacy\cite{snover2009Fluency}.           

While many advancements have been made in MT and QE, there has been limited work studying the usability of these tools to humans. In particular, there is a lack of tools available to make QE accessible to non-MT experts. Avramidis created a GUI to make QE more accessible to non-experts, however users must still be proficient in Python and the command line\cite{avramidis2017QE}.    

At the IUI conference in 2020 researchers presented a demonstration titled: \textit{XAIT: An Interactive Website for Explainable AI for Text}. We used this work as a starting point for investigating XAI efforts for MT, and unfortunately found that none of the cited work related to XAI for MT\cite{oduor2020XAIT}. In fact, the only usability study we were able to uncover for MT was one by Martindale and Carpuat which investigated how revealing to users errors in fluency and adequacy of MT might change their trust in MT. In this work they found that poor fluency in translations can significantly effect users' trust of MT, but that trust is easily rebuilt\cite{martindaleFluency2018}.       

\subsection{XAI Tools \& Approaches}
Though we were unable to find specific XAI tools for MT, we think it is important to provide an overview of the general approaches that exist. Today’s XAI tools employ a variety of explanation strategies. For example, tools like LIME \cite{RiberoLIME2016}, use a simpler model to approximate the behavior of a complex model \cite{SelbstBarocasIntuitive2018}. Proponents of this “model of the model” \cite{SelbstBarocasIntuitive2018} approach – which is also sometimes referred to as generating approximate models – argue that it can present complex processes in a way that is understandable, flexible and mostly accurate \cite{MittelstadtRussellExplain2019}. %However, some argue that the approach can be misleading \cite{rudin2018stop} as approximate models may imply a false sense of simplicity, allow for improper conclusions, or be used to bolster predetermined narratives \cite{MittelstadtRussellExplain2019} and \cite{herman2017promise}. 

Other tools, such as TCAV \cite{KimTCAV2018} and SHAP \cite{LundbergLeeSHAP2017}, use a different explanation strategy, helping users build intuition about how models work by allowing them to test and explore how different inputs relate to different outputs. Rather than explaining the internal rules of a model, these tools purport to help users determine which factors contributed (most) to a particular output \cite{SelbstBarocasIntuitive2018}. %This approach can help users reason about why a model might have made a specific prediction. For example, “reason codes” — text-based explanations about the importance of different input variables — are common in the context of lending \cite{SelbstBarocasIntuitive2018}. However, critics of this “input-output” approach warn that it ceases to be useful when a model’s output results from complex interactions between many factors \cite{SelbstBarocasIntuitive2018}. Additionally, some warn that analysis of factors contributing to a specific output can lead to improper conclusions about the model’s behavior overall \cite{DoshiVelezAccountability2017}. 

A third explanation strategy advocates for the use of simple models that are naturally more interpretable \cite{rudin2018stop}, at least in cases where interpretability is paramount. These types of models are sometimes referred to as “white box”. % and some of DARPA’s XAI efforts can be categorized under this “design for simplicity” approach \cite{DARPAXAI}. Critics, however, invoke a commonly-cited tradeoff between complexity and accuracy, questioning whether designing for simplicity is possible without a loss in predictive accuracy.

Notably, there is a method missing for our use case. None of the typical techniques presented above would be applicable to MT. We postulate that when utility is the goal, the best way to explain an opaque MT model may be by adding contextual information via a QE model. This method differs from typical methods because if focuses on providing information to \textit{improve the utility} of AI output, as opposed providing information to make users more expert in what is happening under the hood. This is the guiding principle behind VeriCAT.   

