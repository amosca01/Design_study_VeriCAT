\section{Related Work}

\ab{To add: Intro paragraph} 

\subsection{MT} 
\ab{To add: human in the loop MT, info on QE and MT, etc.}

\subsection{XAI Tools \& Approaches}
While efforts to make complex computing systems more understandable have existed for decades \cite{MillerHoweInmates2017}, the recent emphasis on Explainable AI (XAI) has inspired investment in novel research and a new class tools. DARPA, the Defense Advanced Research Projects Agency has operated a dedicated XAI program since 2016 \cite{DARPAXAI} and recent academic publications have proposed a variety of approaches for enhancing the “interpretability” of complex ML systems including LIME \cite{RiberoLIME2016}, TCAV \cite{KimTCAV2018}, SHAP \cite{LundbergLeeSHAP2017}, XNN \cite{vaughanXNN2018}, and “Counterfactual Explanations” \cite{WachterCounterfactual2017}. Within the past few years, several companies have also begun to offer specific capabilities to support XAI, either as a core product (see for example, Fiddler Labs, Optimizing Mind, Kyndi, Untangle), or an add-on feature that supplements existing machine learning platforms (for example, DataRobot, IBM Watson, Algorithmia, Domino, H2O.ai, Google Cloud, Amazon Sage Maker, and Microsoft Azure). 

Today’s XAI tools employ a variety of explanation strategies. For example, tools like LIME \cite{RiberoLIME2016}, use a simpler model to approximate the behavior of a complex model \cite{SelbstBarocasIntuitive2018}. Proponents of this “model of the model” \cite{SelbstBarocasIntuitive2018} approach – which is also sometimes referred to as generating approximate models – argue that it can present complex processes in a way that is understandable, flexible and mostly accurate \cite{MittelstadtRussellExplain2019}. However, some argue that the approach can be misleading \cite{rudin2018stop} as approximate models may imply a false sense of simplicity, allow for improper conclusions, or be used to bolster predetermined narratives \cite{MittelstadtRussellExplain2019} and \cite{herman2017promise}. 

Other tools, such as TCAV \cite{KimTCAV2018} and SHAP \cite{LundbergLeeSHAP2017}, use a different explanation strategy, helping users build intuition about how models work by allowing them to test and explore how different inputs relate to different outputs. Rather than explaining the internal rules of a model, these tools purport to help users determine which factors contributed (most) to a particular output \cite{SelbstBarocasIntuitive2018}. This approach can help users reason about why a model might have made a specific prediction. For example, “reason codes” — text-based explanations about the importance of different input variables — are common in the context of lending \cite{SelbstBarocasIntuitive2018}. However, critics of this “input-output” approach warn that it ceases to be useful when a model’s output results from complex interactions between many factors \cite{SelbstBarocasIntuitive2018}. Additionally, some warn that analysis of factors contributing to a specific output can lead to improper conclusions about the model’s behavior overall \cite{DoshiVelezAccountability2017}. 

A third explanation strategy advocates for the use of simple models that are naturally more interpretable \cite{rudin2018stop}, at least in cases where interpretability is paramount. These types of models are sometimes referred to as “white box” and some of DARPA’s XAI efforts can be categorized under this “design for simplicity” approach \cite{DARPAXAI}. Critics, however, invoke a commonly-cited tradeoff between complexity and accuracy, questioning whether designing for simplicity is possible without a loss in predictive accuracy.

\ab{To add: sum it all up} 

%\subsection{Why is Visualization important for XAI?}
%One common theme across all three strategies is enthusiasm for visualizations. Intuitively, a visual representation can make a complex system more understandable. For example, decision trees are viewed as interpretable, in part, because they can be represented via clear, visual representations \cite{FriedmanElements2001}. For more complex models, interactive visualizations are often the interface that permits a user to explore how changing model inputs or parameters produces different outputs, results, or predictions \cite{HallGill2018} and \cite{bastani2017interpreting}. Visualizations may also be particularly helpful in efforts to “democratize” AI, or to make aspects of complex systems accessible to people without substantial technical expertise in AI or ML (CITE?).

%Despite their potential, however, XAI visualizations do not always provide the (experience of) explainability that users seek. Some visualizations are cumbersome to the point that they do not aid interpretability \cite{KimTCAV2018}. Other visualizations provide so much explanatory information that users may become overwhelmed or distracted away from the insight they need \cite{PoursabziManipulating2018}. There are also various ways in which visualizations can be misleading, for example by oversimplifying how a model actually works \cite{adebayo2018sanity}. And some visualizations simply “miss the mark,” representing or emphasizing aspects of a system that are not of primary interest to those viewing or interacting with the tool. Understanding when, how, and why XAI visualizations fall short is an opportunity to identify gaps in current XAI efforts and inform the way we design and evaluate future XAI tools.

%\section{What's missing from current XAI tools?}

%Conspicuously absent from many of today’s XAI efforts is a focus on the user — Who, exactly, is a particular explanation intended for? Does it provide that person with the information they need? Is the explanation presented in a way that this person can easily understand and make sense of it? How can we evaluate the explanation — both its content and its form — to determine whether (or to what extent) it helps a person perform some task, take some action, or understand some insight?

%\textbf{Explanations that contextualize model output for non-technical users.}
%Conspicuously absent from many of today’s XAI efforts is a focus on the user — Who, exactly, is a particular explanation intended for? Does it provide that person with the information they need? Is the explanation presented in a way that this person can easily understand and make sense of it? How can we evaluate the explanation — both its content and its form — to determine whether (or to what extent) it helps a person perform some task, take some action, or understand some insight?

%In prior work~\cite{brennen2020What} we discuss results from a qualitative research effort we conducted to understand how various stakeholders across industry and government characterize the problem of XAI. Between March and December 2019, we conducted 40 interviews and 2 focus groups with stakeholders from government, academia, and industry. Through these discussions, we identified several unmet XAI needs that would benefit from additional research attention. 

%Instead of detailed explanations of model mechanics, many end users of AI/ML systems report needing explanations that help them use model output more effectively. Many of these users do not have substantial expertise in ML or AI; instead, they are experts in the data. And often, they do not have the time, the technical expertise, or the interest to delve deep into how models work. Through our stakeholder interviews, end users of AI systems told us they wanted explanations that gave them contextual information that helps them assess model output and answer questions such as: Should they trust the output of this particular model in this particular case? In what ways might this output be biased? What kind of errors does this model typically make? And how likely is it that this particular output is in error?

%\textbf{Emphasis on the design and usability of XAI interfaces.} 
%Extracting accurate, informative and relevant information from AI/ML models is only part of the challenge of explainability. This information must also be presented through an interface — visual or otherwise — that is clear, usable, and effective. However, the interface design aspects of XAI are often overlooked. Perhaps teams lack access to UI/UX design expertise or underestimate the challenge of designing an intuitive, user-friendly XAI interface. But de-prioritizing interface design predictably results in the development of tools that either do not meet end-users’ primary needs or that are confusing and frustrating to use. Additional emphasis on usability can help teams create XAI tools that are more accessible and satisfying for users, but also that present information in a way that is more actionable, particularly for non-technical users.

%\textbf{Evaluation criteria that mirror users’ needs.}
%It is common practice to evaluate machine learning models — exclusively — on their predictive accuracy. For additional evaluation criteria, ML teams might look to system performance (i.e. processing speed). Though these criteria have the benefit of being easily quantifiable, they are insufficient for evaluation of XAI tools and systems. It is easy to imagine how an XAI tool might generate an explanation that is both accurate and quick to produce, but that a user fails to understand, see the relevance of, or act on.

%It isn’t obvious how to quantify — let alone measure — the effectiveness of an explanation (machine-generated or otherwise). While this question is by no means settled, the robust literature on evaluating visualizations through controlled user studies provides an invaluable methodological starting point (CITE?). Additionally, DARPA’s XAI program provides a model for how to conduct user-centered evaluations of XAI tools. Tools are evaluated in the context of a controlled user study, during which participants perform a specific task. Some participants use the XAI tool to perform the task and others do not, allowing statistically significant differences in users’ performance to be attributed to (explanations provided by) the tool. In addition to users’ performance on the task, DARPA’s XAI evaluations incorporate a variety of other user-centered dependent measures including users’: time to complete the task, awareness of a model’s limitations, cognitive load, satisfaction, and trust (CITE). Some of the preliminary findings from DARPA’s XAI evaluations are surprising. In several cases the XAI tools evaluated did not seem to improve users’ performance on the specified task. However, users seemed to overwhelming prefer explanations, even when they did not lead to improved performance, and even when the explanations were incorrect (CITE).

%\subsection{What is the solution?}
%\textbf{User-centered XAI can help to address some of the shortcomings of today’s XAI tools}. An iterative design process can help to improve the usability of visualizations and to foreground what types of explanations, based on which model features, are of greatest interest to users. Using an opaque model to expose the limitations of another opaque model is one way to provide useful contextual information — about when a model is likely to fail, and what is as stake when it does — without misleading users or oversimplifying the mechanics of how models work. And by building on DARPA’s XAI evaluations, we show how to measure the utility of XAI tools in a controlled user study. Given the broad desire for transparency and the many possible use cases for XAI, future tools will likely require a variety of explanation strategies. To be effective, they will also need to be designed and evaluated in relation to a particular application or use case. 