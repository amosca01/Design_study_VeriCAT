\section{Lessons Learned}

Through the process of conceptualizing, testing, and refining VeriCAT we show that QE can provide context to translations and help users to appropriately trust MT text.  
Below, we present several lessons learned from the evaluation of VeriCAT.     

\paragraph{\textbf{Test for behaviors and beliefs}} In this study we saw that adding quality scores to MT text did not significantly change participants' self-reported trust of MT. However, we did see a significant change in behavior when participants were exposed to quality scores. Moreover, we observe that participants tend to opt for no re-translation when presented with a passage of machine translated text and no additional information, which suggests they may believe re-translation is unnecessary, or may be unwilling to choose without sufficient information. In contrast, we observe participants in quality score conditions tend to opt for a human re-translation. This suggests a more appropriate level of trust in the MT output, as participants are choosing to double check portions of it. Based on this mismatch of self-reported trust and behavior, we recommend testing for changes in behavior as well as collecting self-reported measures when evaluating the performance of tools designed to inform users' trust.     

\paragraph{\textbf{Plan for overconfidence}} Interestingly, we found negative correlations between participants' usage of MT tools and overall scores and between participants' self-rated expertise and overall scores. Our results imply that it may be necessary to make indications of poor translation quality more alarming to users who use MT more often. Similarly, users who have high confidence in their AI and MT expertise may need intensified warnings of poor translation quality compared to users with low confidence in these areas. In short, our results suggest that there is likely not a ``one size fits all" solution to helping users have appropriate trust of MT text; in addition to customizing such tools on a macro-level for various domains they should be customized on a micro-level to account for differences in individual users.   

%\paragraph{\textbf{Employ a user-centered design process}} Perhaps the most important lesson learned in building and evaluating VeriCAT is that a user-centered design process is invaluable to XAI development. Through this experiment we were able to establish that sentence-level quality scores are a viable option for improving analysts' performance in identifying poor quality MT text. However, as stated earlier, there are multiple ways in which translation quality could be communicated to users, and we would strongly urge future work to look into other options. Further, we believe our study is an excellent example of how the outcome of usability testing for XAI can inform interface design, and also model development. For example, if we had found that DA scores significantly improved participants' performance but QE scores did not, before abandoning QE as an explanation option we would have iterated on the QE model to see if making it more accurate (i.e. closer to ground truth) could reduce the difference in performance between participants using VeriCAT with DA vs QE scores.     


%Revisions to interface (\textbf{interface version 2})

%Insights about model design (\textbf{model version 3})
