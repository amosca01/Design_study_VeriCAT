\section{Lessons Learned}

Through the process of conceptualizing, testing, and refining VeriCAT we show that MT output can be thought of as an XAI problem, and QE can be used to provide context to translations to guide users towards more appropriate trust of output. Below, we present several lessons learned from the evaluation of VeriCAT.     

\paragraph{\textbf{Test idealized explanations}} In Experiment 1 we test idealized versions of two explanation techniques for MT (word-level predicted errors and sentence-level quality estimations). While models exist for each technique, neither are perfect. One of our goals with Experiment 1 was to determine if either of these particular explanation techniques were useful enough to users to be worth further developing. 

We used this ``idealized" method to determine a goal post for each explanation technique. The thought behind this is that if the perfect hand crafted versions of either technique did not significantly improve participants' performance then that would be a clear indication that the technique was not worth further refinement. This happened with the word-level predicted error; which we found did not significantly improve participants' performance over No XAI. Thanks to this outcome we were able to refine our second experiment and remove the word-level predicted error condition from it. This saved time, effort, and money in the iterative development of VeriCAT.   

\paragraph{\textbf{Test for behaviors and beliefs}} According to prior work, one of the things people are asking for when they ask for XAI is an indication of whether they should trust a model or model output\cite{brennen2020What}. In this study we saw that adding XAI to MT did not significantly change participants' self-reported trust of MT. However, we did see a significant change in behavior when participants were exposed to XAI. In our experiments we observed that participants tend to opt for no re-translation when presented with a passage of machine translated text and no additional information. This suggests that participants may be over-trusting of the MT output, and see no reason to question any of the translations they see. In contrast, we observe participants in effective XAI conditions tend to opt for a human re-translation. This suggests a more appropriate level of trust in the MT output, as participants are choosing to double check portions of it. Based on this mismatch of self-reported trust and behavior, we recommend testing for changes in behavior as well as collecting self-reported measures as part of the the user-centered XAI process.    

\paragraph{\textbf{Plan for overconfidence}} Interestingly, we found negative correlations between participants' usage of MT tools and overall scores and between participants' self-rated expertise and overall scores. While we are well aware that correlation does not necessarily imply causation, we do strongly urge XAI designers to take this finding into consideration when designing tools. Our results imply that it may be necessary to make indications of poor translation quality more alarming to users who use MT more often. Similarly, users who have high confidence in their AI and MT expertise may need intensified warnings of poor translation quality compared to users with low confidence in these areas. In short, our results suggest that there is likely not a ``one size fits all" solution to XAI problem; in addition to customizing XAI design on a macro-level for various domains it should be customized on a micro-level to account for differences in individual users.   

\paragraph{\textbf{Employ a user-centered design process}} Perhaps the most important lesson learned in building and evaluating VeriCAT is that a user-centered design process is critical to XAI development. Through this experiment we were able to establish that sentence-level quality scores are a viably option for improving analysts' performance in identifying poor quality MT text. However, there are additional ways in which translation quality could be communicated to users, and we would strongly urger future work to continue the iterative process we have stared here and to test and develop more XAI systems for MT.   

%Revisions to interface (\textbf{interface version 2})

%Insights about model design (\textbf{model version 3})
